import json
import numpy as np
import faiss
from text2vec import SentenceModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# åŠ è½½åŸæ–‡å’Œå‘é‡ç´¢å¼•
texts = json.load(open("text_map.json", encoding="utf-8"))
index = faiss.read_index("faiss_index.bin")

# åŠ è½½å‘é‡æ¨¡å‹
model_embed = SentenceModel("shibing624/text2vec-base-multilingual")

# åŠ è½½è¯­è¨€æ¨¡å‹ï¼ˆQwenï¼‰
model_id = "Qwen/Qwen1.5-0.5B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).eval().to("cuda" if torch.cuda.is_available() else "cpu")

# æ£€ç´¢å‡½æ•°
def retrieve_context(query, top_k=3):
    vec = model_embed.encode([query])[0]
    D, I = index.search(np.array([vec]).astype("float32"), k=top_k)
    return "\n".join([texts[i] for i in I[0]])

# âœ… å®šä¹‰ç­”é¢˜å‡½æ•°ï¼ˆå¿…é¡»å’Œ app.py å¯¼å…¥åç§°ä¸€è‡´ï¼‰
def answer_question(query):
    context = retrieve_context(query)
    if not context.strip():
        return "âš ï¸ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ì˜ë£Œ ì§€ì‹ì´ í’ë¶€í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì§ˆë¬¸: {query}
ë‹µë³€:"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7
    )
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    if "ë‹µë³€:" in result:
        return "ğŸ¤– ë‹µë³€: " + result.split("ë‹µë³€:")[-1].strip()
    return "ğŸ¤– ë‹µë³€: " + result.strip()
