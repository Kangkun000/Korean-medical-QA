import json
import numpy as np
import faiss
from text2vec import SentenceModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

#  ì†ŒìŠ¤ í…ìŠ¤íŠ¸ì™€ ë²¡í„° ì¸ë±ìŠ¤ ë¡œë”©
texts = json.load(open("text_map.json", encoding="utf-8"))
index = faiss.read_index("faiss_index.bin")

# ë²¡í„° ëª¨ë¸ ë¡œë”©
model_embed = SentenceModel("shibing624/text2vec-base-multilingual")

# ì–¸ì–´ ëª¨ë¸(Qwen) ë¡œë”© ì¤‘
model_id = "Qwen/Qwen1.5-0.5B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).eval().to("cuda" if torch.cuda.is_available() else "cpu")

# ê²€ìƒ‰ ê¸°ëŠ¥
def retrieve_context(query, top_k=3):
    vec = model_embed.encode([query])[0]
    D, I = index.search(np.array([vec]).astype("float32"), k=top_k)
    return "\n".join([texts[i] for i in I[0]])

#     âœ… answer í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤(app.pyì˜ ê°€ì ¸ì˜¤ê¸° ì´ë¦„ê³¼ ì¼ê´€ì„±ì´ ìˆì–´ì•¼ í•¨)
def answer_question(query):
    context = retrieve_context(query)
    if not context.strip():
        return "âš ï¸ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ì˜ë£Œ ì§€ì‹ì´ í’ë¶€í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

ì •ë³´:
{context}

ì§ˆë¬¸: {query}
ë‹µë³€:"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7
    )
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    if "ë‹µë³€:" in result:
        return "ğŸ¤– ë‹µë³€: " + result.split("ë‹µë³€:")[-1].strip()
    return "ğŸ¤– ë‹µë³€: " + result.strip()
